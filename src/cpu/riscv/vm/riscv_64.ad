//
// Copyright (c) 2003, 2013, Oracle and/or its affiliates. All rights reserved.
// Copyright (c) 2015, 2018, Loongson Technology. All rights reserved.
// DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
//
// This code is free software; you can redistribute it and/or modify it
// under the terms of the GNU General Public License version 2 only, as
// published by the Free Software Foundation.
//
// This code is distributed in the hope that it will be useful, but WITHOUT
// ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
// FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
// version 2 for more details (a copy is included in the LICENSE file that
// accompanied this code).
//
// You should have received a copy of the GNU General Public License version
// 2 along with this work; if not, write to the Free Software Foundation,
// Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
//
// Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
// or visit www.oracle.com if you need additional information or have any
// questions.
//
//

// GodSon3 Architecture Description File

//----------REGISTER DEFINITION BLOCK------------------------------------------
// This information is used by the matcher and the register allocator to
// describe individual registers and classes of registers within the target
// archtecture.

// format:
// reg_def name (call convention, c-call convention, ideal type, encoding);
//     call convention :
//      NS  = No-Save
//      SOC = Save-On-Call
//      SOE = Save-On-Entry
//      AS  = Always-Save
//    ideal type :
//      see opto/opcodes.hpp for more info
// reg_class name (reg, ...);
// alloc_class name (reg, ...);
register %{

// General Registers
// Integer Registers
  reg_def X0    ( NS,  NS,  Op_RegI,  0, VMRegImpl::Bad());       // hard-wired zero / zero
  reg_def X1    ( NS,  NS,  Op_RegI,  1, X1->as_VMReg());         // return address  / ra
  reg_def X1_H  ( NS,  NS,  Op_RegI,  1, X1->as_VMReg()->next());
  reg_def X2    (SOC, SOC,  Op_RegI,  2, X2->as_VMReg());         // stack pointer   / sp
  reg_def X2_H  (SOC, SOC,  Op_RegI,  2, X2->as_VMReg()->next());
  reg_def X3    (SOC, SOC,  Op_RegI,  3, X3->as_VMReg());         // global pointer  / gp
  reg_def X3_H  (SOC, SOC,  Op_RegI,  3, X3->as_VMReg()->next());
  reg_def X4    (SOC, SOC,  Op_RegI,  4, X4->as_VMReg());         // thread pointer  / tp
  reg_def X4_H  (SOC, SOC,  Op_RegI,  4, X4->as_VMReg()->next());
  reg_def X5    (SOC, SOC,  Op_RegI,  5, X5->as_VMReg());         // temp register/alternative link register / t0
  reg_def X5_H  (SOC, SOC,  Op_RegI,  5, X5->as_VMReg()->next());
  reg_def X6    (SOC, SOC,  Op_RegI,  6, X6->as_VMReg());         // temp register / t1
  reg_def X6_H  (SOC, SOC,  Op_RegI,  6, X6->as_VMReg()->next());
  reg_def X7    (SOC, SOC,  Op_RegI,  7, X7->as_VMReg());         // temp register / t2
  reg_def X7_H  (SOC, SOC,  Op_RegI,  7, X7->as_VMReg()->next());
  reg_def X8    (SOC, SOC,  Op_RegI,  8, X8->as_VMReg());         // frame pointer/saved register / s0/fp
  reg_def X8_H  (SOC, SOC,  Op_RegI,  8, X8->as_VMReg()->next());
  reg_def X9    (SOC, SOC,  Op_RegI,  9, X9->as_VMReg());         // saved register               / s1
  reg_def X9_H  (SOC, SOC,  Op_RegI,  9, X9->as_VMReg()->next());
  reg_def X10   (SOC, SOC,  Op_RegI,  10, X10->as_VMReg());        // function argument/return value / a0
  reg_def X10_H (SOC, SOC,  Op_RegI,  10, X10->as_VMReg()->next());
  reg_def X11   (SOC, SOC,  Op_RegI,  11, X11->as_VMReg());        // function argument/return value / a1
  reg_def X11_H (SOC, SOC,  Op_RegI,  11, X11->as_VMReg()->next());
  reg_def X12   (SOC, SOC,  Op_RegI,  12, X12->as_VMReg());        // function argument              / a2
  reg_def X12_H (SOC, SOC,  Op_RegI,  12, X12->as_VMReg()->next());
  reg_def X13   (SOC, SOC,  Op_RegI,  13, X13->as_VMReg());        // function argument              / a3
  reg_def X13_H (SOC, SOC,  Op_RegI,  13, X13->as_VMReg()->next());
  reg_def X14   (SOC, SOC,  Op_RegI,  14, X14->as_VMReg());        // function argument              / a4
  reg_def X14_H (SOC, SOC,  Op_RegI,  14, X14->as_VMReg()->next());
  reg_def X15   (SOC, SOC,  Op_RegI,  15, X15->as_VMReg());        // function argument              / a5
  reg_def X15_H (SOC, SOC,  Op_RegI,  15, X15->as_VMReg()->next());
  reg_def X16   (SOC, SOE,  Op_RegI,  16, X16->as_VMReg());        // function argument              / a6
  reg_def X16_H (SOC, SOE,  Op_RegI,  16, X16->as_VMReg()->next());
  reg_def X17   (SOC, SOE,  Op_RegI,  17, X17->as_VMReg());        // function argument              / a7
  reg_def X17_H (SOC, SOE,  Op_RegI,  17, X17->as_VMReg()->next());
  reg_def X18   (SOC, SOE,  Op_RegI,  18, X18->as_VMReg());        // saved register                 / s2
  reg_def X18_H (SOC, SOE,  Op_RegI,  18, X18->as_VMReg()->next());
  reg_def X19   (SOC, SOE,  Op_RegI,  19, X19->as_VMReg());        // saved register                 / s3
  reg_def X19_H (SOC, SOE,  Op_RegI,  19, X19->as_VMReg()->next());
  reg_def X20   (SOC, SOE,  Op_RegI,  20, X20->as_VMReg());        // saved register                 / s4
  reg_def X20_H (SOC, SOE,  Op_RegI,  20, X20->as_VMReg()->next());
  reg_def X21   (SOC, SOE,  Op_RegI,  21, X21->as_VMReg());        // saved register                 / s5
  reg_def X21_H (SOC, SOE,  Op_RegI,  21, X21->as_VMReg()->next());
  reg_def X22   (SOC, SOE,  Op_RegI,  22, X22->as_VMReg());        // saved register                 / s6
  reg_def X22_H (SOC, SOE,  Op_RegI,  22, X22->as_VMReg()->next());
  reg_def X23   (SOC, SOE,  Op_RegI,  23, X23->as_VMReg());        // saved register                 / s7
  reg_def X23_H (SOC, SOE,  Op_RegI,  23, X23->as_VMReg()->next());
  reg_def X24   (SOC, SOC,  Op_RegI,  24, X24->as_VMReg());        // saved register                 / s8
  reg_def X24_H (SOC, SOC,  Op_RegI,  24, X24->as_VMReg()->next());
  reg_def X25   (SOC, SOC,  Op_RegI,  25, X25->as_VMReg());        // saved register                 / s9
  reg_def X25_H (SOC, SOC,  Op_RegI,  25, X25->as_VMReg()->next());
  reg_def X26   (SOC, SOC,  Op_RegI,  26, X26->as_VMReg());        // saved register                 / s10
  reg_def X26_H (SOC, SOC,  Op_RegI,  26, X26->as_VMReg()->next());
  reg_def X27   (SOC, SOC,  Op_RegI,  27, X27->as_VMReg());        // saved register                 / s11
  reg_def X27_H (SOC, SOC,  Op_RegI,  27, X27->as_VMReg()->next());
  reg_def X28   (SOC, SOC,  Op_RegI,  28, X28->as_VMReg());        // temp register                  / t3
  reg_def X28_H (SOC, SOC,  Op_RegI,  28, X28->as_VMReg()->next());
  reg_def X29   (SOC, SOC,  Op_RegI,  29, X29->as_VMReg());        // temp register                  / t4
  reg_def X29_H (SOC, SOC,  Op_RegI,  29, X29->as_VMReg()->next());
  reg_def X30   (SOC, SOC,  Op_RegI,  30, X30->as_VMReg());        // temp register                  / t5
  reg_def X30_H (SOC, SOC,  Op_RegI,  30, X30->as_VMReg()->next());
  reg_def X31   (SOC, SOC,  Op_RegI,  31, X31->as_VMReg());        // temp register                  / t6
  reg_def X31_H (SOC, SOC,  Op_RegI,  31, X31->as_VMReg()->next());

// Floating registers.
  reg_def F0          ( SOC, SOC, Op_RegF, 0, F0->as_VMReg());
  reg_def F0_H        ( SOC, SOC, Op_RegF, 0, F0->as_VMReg()->next());
  reg_def F1          ( SOC, SOC, Op_RegF, 1, F1->as_VMReg());
  reg_def F1_H        ( SOC, SOC, Op_RegF, 1, F1->as_VMReg()->next());
  reg_def F2          ( SOC, SOC, Op_RegF, 2, F2->as_VMReg());
  reg_def F2_H        ( SOC, SOC, Op_RegF, 2, F2->as_VMReg()->next());
  reg_def F3          ( SOC, SOC, Op_RegF, 3, F3->as_VMReg());
  reg_def F3_H        ( SOC, SOC, Op_RegF, 3, F3->as_VMReg()->next());
  reg_def F4          ( SOC, SOC, Op_RegF, 4, F4->as_VMReg());
  reg_def F4_H        ( SOC, SOC, Op_RegF, 4, F4->as_VMReg()->next());
  reg_def F5          ( SOC, SOC, Op_RegF, 5, F5->as_VMReg());
  reg_def F5_H        ( SOC, SOC, Op_RegF, 5, F5->as_VMReg()->next());
  reg_def F6          ( SOC, SOC, Op_RegF, 6, F6->as_VMReg());
  reg_def F6_H        ( SOC, SOC, Op_RegF, 6, F6->as_VMReg()->next());
  reg_def F7          ( SOC, SOC, Op_RegF, 7, F7->as_VMReg());
  reg_def F7_H        ( SOC, SOC, Op_RegF, 7, F7->as_VMReg()->next());
  reg_def F8          ( SOC, SOC, Op_RegF, 8, F8->as_VMReg());
  reg_def F8_H        ( SOC, SOC, Op_RegF, 8, F8->as_VMReg()->next());
  reg_def F9          ( SOC, SOC, Op_RegF, 9, F9->as_VMReg());
  reg_def F9_H        ( SOC, SOC, Op_RegF, 9, F9->as_VMReg()->next());
  reg_def F10         ( SOC, SOC, Op_RegF, 10, F10->as_VMReg());
  reg_def F10_H       ( SOC, SOC, Op_RegF, 10, F10->as_VMReg()->next());
  reg_def F11         ( SOC, SOC, Op_RegF, 11, F11->as_VMReg());
  reg_def F11_H       ( SOC, SOC, Op_RegF, 11, F11->as_VMReg()->next());
  reg_def F12         ( SOC, SOC, Op_RegF, 12, F12->as_VMReg());
  reg_def F12_H       ( SOC, SOC, Op_RegF, 12, F12->as_VMReg()->next());
  reg_def F13         ( SOC, SOC, Op_RegF, 13, F13->as_VMReg());
  reg_def F13_H       ( SOC, SOC, Op_RegF, 13, F13->as_VMReg()->next());
  reg_def F14         ( SOC, SOC, Op_RegF, 14, F14->as_VMReg());
  reg_def F14_H       ( SOC, SOC, Op_RegF, 14, F14->as_VMReg()->next());
  reg_def F15         ( SOC, SOC, Op_RegF, 15, F15->as_VMReg());
  reg_def F15_H       ( SOC, SOC, Op_RegF, 15, F15->as_VMReg()->next());
  reg_def F16         ( SOC, SOC, Op_RegF, 16, F16->as_VMReg());
  reg_def F16_H       ( SOC, SOC, Op_RegF, 16, F16->as_VMReg()->next());
  reg_def F17         ( SOC, SOC, Op_RegF, 17, F17->as_VMReg());
  reg_def F17_H       ( SOC, SOC, Op_RegF, 17, F17->as_VMReg()->next());
  reg_def F18         ( SOC, SOC, Op_RegF, 18, F18->as_VMReg());
  reg_def F18_H       ( SOC, SOC, Op_RegF, 18, F18->as_VMReg()->next());
  reg_def F19         ( SOC, SOC, Op_RegF, 19, F19->as_VMReg());
  reg_def F19_H       ( SOC, SOC, Op_RegF, 19, F19->as_VMReg()->next());
  reg_def F20         ( SOC, SOC, Op_RegF, 20, F20->as_VMReg());
  reg_def F20_H       ( SOC, SOC, Op_RegF, 20, F20->as_VMReg()->next());
  reg_def F21         ( SOC, SOC, Op_RegF, 21, F21->as_VMReg());
  reg_def F21_H       ( SOC, SOC, Op_RegF, 21, F21->as_VMReg()->next());
  reg_def F22         ( SOC, SOC, Op_RegF, 22, F22->as_VMReg());
  reg_def F22_H       ( SOC, SOC, Op_RegF, 22, F22->as_VMReg()->next());
  reg_def F23         ( SOC, SOC, Op_RegF, 23, F23->as_VMReg());
  reg_def F23_H       ( SOC, SOC, Op_RegF, 23, F23->as_VMReg()->next());
  reg_def F24         ( SOC, SOC, Op_RegF, 24, F24->as_VMReg());
  reg_def F24_H       ( SOC, SOC, Op_RegF, 24, F24->as_VMReg()->next());
  reg_def F25         ( SOC, SOC, Op_RegF, 25, F25->as_VMReg());
  reg_def F25_H       ( SOC, SOC, Op_RegF, 25, F25->as_VMReg()->next());
  reg_def F26         ( SOC, SOC, Op_RegF, 26, F26->as_VMReg());
  reg_def F26_H       ( SOC, SOC, Op_RegF, 26, F26->as_VMReg()->next());
  reg_def F27         ( SOC, SOC, Op_RegF, 27, F27->as_VMReg());
  reg_def F27_H       ( SOC, SOC, Op_RegF, 27, F27->as_VMReg()->next());
  reg_def F28         ( SOC, SOC, Op_RegF, 28, F28->as_VMReg());
  reg_def F28_H       ( SOC, SOC, Op_RegF, 28, F28->as_VMReg()->next());
  reg_def F29         ( SOC, SOC, Op_RegF, 29, F29->as_VMReg());
  reg_def F29_H       ( SOC, SOC, Op_RegF, 29, F29->as_VMReg()->next());
  reg_def F30         ( SOC, SOC, Op_RegF, 30, F30->as_VMReg());
  reg_def F30_H       ( SOC, SOC, Op_RegF, 30, F30->as_VMReg()->next());
  reg_def F31         ( SOC, SOC, Op_RegF, 31, F31->as_VMReg());
  reg_def F31_H       ( SOC, SOC, Op_RegF, 31, F31->as_VMReg()->next());


// ----------------------------
// Special Registers
// Condition Codes Flag Registers
  reg_def RISCV_FLAG (SOC, SOC,  Op_RegFlags, 1, as_Register(1)->as_VMReg());
//S6 is used for get_thread(S6)
//S5 is uesd for heapbase of compressed oop
alloc_class chunk0(
                     X1, X1_H,
                     X2, X2_H,
                     X3, X3_H,
                     X4, X4_H,
                     X5, X5_H,
                     X6, X6_H,
                     X7, X7_H,
                     X8, X8_H,
                     X9, X9_H,
                     X10, X10_H,
                     X11, X11_H,
                     X12, X12_H,
                     X13, X13_H,
                     X14, X14_H,
                     X15, X15_H,
                     X16, X16_H,
                     X17, X17_H,
                     X18, X18_H,
                     X19, X19_H,
                     X20, X20_H,
                     X21, X21_H,
                     X22, X22_H,
                     X23, X23_H,
                     X24, X24_H,
                     X25, X25_H,
                     X26, X26_H,
                     X27, X27_H,
                     X28, X28_H,
                     X29, X29_H,
                     X30, X30_H,
                     X31, X31_H
                 );

alloc_class chunk1(  F0, F0_H,
                     F1, F1_H,
                     F2, F2_H,
                     F3, F3_H,
                     F4, F4_H,
                     F5, F5_H,
                     F6, F6_H,
                     F7, F7_H,
                     F8, F8_H,
                     F9, F9_H,
                     F10, F10_H,
                     F11, F11_H,
                     F20, F20_H,
                     F21, F21_H,
                     F22, F22_H,
                     F23, F23_H,
                     F24, F24_H,
                     F25, F25_H,
                     F26, F26_H,
                     F27, F27_H,
                     F28, F28_H,
                     F19, F19_H,
                     F18, F18_H,
                     F17, F17_H,
                     F16, F16_H,
                     F15, F15_H,
                     F14, F14_H,
                     F13, F13_H,
                     F12, F12_H,
                     F29, F29_H,
                     F30, F30_H,
                     F31, F31_H);

alloc_class chunk2(RISCV_FLAG);

reg_class s_reg( X8, X9, X18, X19, X20, X21, X22, X23, X24, X25, X26, X27 );
reg_class s0_reg( X8 );
reg_class s1_reg( X9 );
reg_class s2_reg( X18 );
reg_class s3_reg( X19 );
reg_class s4_reg( X20 );
reg_class s5_reg( X21 );
reg_class s6_reg( X22 );
reg_class s7_reg( X23 );
reg_class s8_reg( X24 );
reg_class s9_reg( X25 );
reg_class s10_reg( X26 );
reg_class s11_reg( X27 );

reg_class t_reg( X5, X6, X7, X28, X29, X30, X31 );
reg_class t0_reg( X5 );
reg_class t1_reg( X6 );
reg_class t2_reg( X7 );
reg_class t3_reg( X28 );
reg_class t4_reg( X29 );
reg_class t5_reg( X30 );
reg_class t6_reg( X31 );

reg_class a_reg( X10, X11, X12, X13, X14, X15, X16, X17 );
reg_class a0_reg( X10 );
reg_class a1_reg( X11 );
reg_class a2_reg( X12 );
reg_class a3_reg( X13 );
reg_class a4_reg( X14 );
reg_class a5_reg( X15 );
reg_class a6_reg( X16 );
reg_class a7_reg( X17 );

reg_class sp_reg( X2, X2_H );
reg_class fp_reg( X8, X8_H );

reg_class riscv_flags(RISCV_FLAG);

reg_class a0_long_reg( X10, X10_H );
reg_class a1_long_reg( X11, X11_H );
reg_class a2_long_reg( X12, X12_H );
reg_class a3_long_reg( X13, X13_H );
reg_class a4_long_reg( X14, X14_H );
reg_class a5_long_reg( X15, X15_H );
reg_class a6_long_reg( X16, X16_H );
reg_class a7_long_reg( X17, X17_H );
reg_class t0_long_reg( X5, X5_H );
reg_class t1_long_reg( X6, X6_H );
reg_class t2_long_reg( X7, X7_H );
reg_class t3_long_reg( X28, X28_H );
reg_class t4_long_reg( X29, X29_H );
reg_class t5_long_reg( X30, X30_H );
reg_class t6_long_reg( X31, X31_H );
reg_class s0_long_reg( X8, X8_H );
reg_class s1_long_reg( X9, X9_H );
reg_class s2_long_reg( X18, X18_H );
reg_class s3_long_reg( X19, X19_H );
reg_class s4_long_reg( X20, X20_H );
reg_class s5_long_reg( X21, X21_H );
reg_class s6_long_reg( X22, X22_H );
reg_class s7_long_reg( X23, X23_H );
reg_class s8_long_reg( X24, X24_H );
reg_class s9_long_reg( X25, X25_H );
reg_class s10_long_reg( X26, X26_H );
reg_class s11_long_reg( X27, X27_H );

reg_class int_reg( X10, X11, X12, X13, X14, X15, X16, X17, X5, X6, X7, X28, X29, X30, X31, X8, X9, X18, X19, X20, X21, X22, X23, X24, X25, X26, X27 );

reg_class no_Ax_int_reg( X5, X6, X7, X28, X29, X30, X31, X8, X9, X18, X19, X20, X21, X22, X23, X24, X25, X26, X27);


// Floating point registers.
// F31 are not used as temporary registers in D2I
reg_class flt_reg( F0, F1, F2, F3, F4, F5, F6, F7, F8, F9, F10, F11, F12, F13, F14, F15, F16, F17 F18, F19, F20, F21, F22, F23, F24, F25, F26, F27, F28, F29, F31);
reg_class dbl_reg( F0, F0_H,
                   F1, F1_H,
                   F2, F2_H,
                   F3, F3_H,
                   F4, F4_H,
                   F5, F5_H,
                   F6, F6_H,
                   F7, F7_H,
                   F8, F8_H,
                   F9, F9_H,
                   F10, F10_H,
                   F11, F11_H,
                   F12, F12_H,
                   F13, F13_H,
                   F14, F14_H,
                   F15, F15_H,
                   F16, F16_H,
                   F17, F17_H,
                   F18, F18_H,
                   F19, F19_H,
                   F20, F20_H,
                   F21, F21_H,
                   F22, F22_H,
                   F23, F23_H,
                   F24, F24_H,
                   F25, F25_H,
                   F26, F26_H,
                   F27, F27_H,
                   F28, F28_H,
                   F29, F29_H,
                   F31, F31_H);

reg_class flt_arg0( F12 );
reg_class dbl_arg0( F12, F12_H );
reg_class dbl_arg1( F14, F14_H );

%}

//----------DEFINITION BLOCK---------------------------------------------------
// Define name --> value mappings to inform the ADLC of an integer valued name
// Current support includes integer values in the range [0, 0x7FFFFFFF]
// Format:
//        int_def  <name>         ( <int_value>, <expression>);
// Generated Code in ad_<arch>.hpp
//        #define  <name>   (<expression>)
//        // value == <int_value>
// Generated code in ad_<arch>.cpp adlc_verification()
//        assert( <name> == <int_value>, "Expect (<expression>) to equal <int_value>");
//
definitions %{
  int_def DEFAULT_COST      (    100,     100);
  int_def HUGE_COST         (1000000, 1000000);

  // Memory refs are twice as expensive as run-of-the-mill.
  int_def MEMORY_REF_COST   (    200, DEFAULT_COST * 2);

  // Branches are even more expensive.
  int_def BRANCH_COST       (    300, DEFAULT_COST * 3);
  // we use jr instruction to construct call, so more expensive
  int_def CALL_COST         (    500, DEFAULT_COST * 5);
/*
        int_def EQUAL             (   1, 1  );
        int_def NOT_EQUAL         (   2, 2  );
        int_def GREATER           (   3, 3  );
        int_def GREATER_EQUAL     (   4, 4  );
        int_def LESS              (   5, 5  );
        int_def LESS_EQUAL        (   6, 6  );
*/
%}



//----------SOURCE BLOCK-------------------------------------------------------
// This is a block of C++ code which provides values, functions, and
// definitions necessary in the rest of the architecture description

source_hpp %{
// Header information of the source block.
// Method declarations/definitions which are used outside
// the ad-scope can conveniently be defined here.
//
// To keep related declarations/definitions/uses close together,
// we switch between source %{ }% and source_hpp %{ }% freely as needed.

class CallStubImpl {

  //--------------------------------------------------------------
  //---<  Used for optimization in Compile::shorten_branches  >---
  //--------------------------------------------------------------

 public:
  // Size of call trampoline stub.
  static uint size_call_trampoline() {
    return 0; // no call trampolines on this platform
  }

  // number of relocations needed by a call trampoline stub
  static uint reloc_call_trampoline() {
    return 0; // no call trampolines on this platform
  }
};

class HandlerImpl {

 public:

  static int emit_exception_handler(CodeBuffer &cbuf);
  static int emit_deopt_handler(CodeBuffer& cbuf);

  static uint size_exception_handler() {
    // NativeCall instruction size is the same as NativeJump.
    // exception handler starts out as jump and can be patched to
    // a call be deoptimization.  (4932387)
    // Note that this value is also credited (in output.cpp) to
    // the size of the code section.
    int size = NativeCall::instruction_size;
    return round_to(size, 16);
  }

#ifdef _LP64
  static uint size_deopt_handler() {
    int size = NativeCall::instruction_size;
    return round_to(size, 16);
  }
#else
  static uint size_deopt_handler() {
    // NativeCall instruction size is the same as NativeJump.
    // exception handler starts out as jump and can be patched to
    // a call be deoptimization.  (4932387)
    // Note that this value is also credited (in output.cpp) to
    // the size of the code section.
    return 5 + NativeJump::instruction_size; // pushl(); jmp;
  }
#endif
};

%} // end source_hpp

source %{

#define   NO_INDEX    0
#define   RELOC_IMM64    Assembler::imm_operand
#define   RELOC_DISP32   Assembler::disp32_operand


#define __ _masm.


// Emit exception handler code.
// Stuff framesize into a register and call a VM stub routine.
int HandlerImpl::emit_exception_handler(CodeBuffer& cbuf) {
  // Note that the code buffer's insts_mark is always relative to insts.
  // That's why we must use the macroassembler to generate a handler.
  MacroAssembler _masm(&cbuf);
  address base = __ start_a_stub(size_exception_handler());
  if (base == NULL) {
    ciEnv::current()->record_failure("CodeCache is full");
    return 0;  // CodeBuffer::expand failed
  }

  int offset = __ offset();

  __ block_comment("; emit_exception_handler");

  cbuf.set_insts_mark();
//  __ relocate(relocInfo::runtime_call_type);
  __ jump64_patchable((address)OptoRuntime::exception_blob()->entry_point(), relocInfo::runtime_call_type, false);
  __ align(16);
  assert(__ offset() - offset <= (int) size_exception_handler(), "overflow");
  __ end_a_stub();
  return offset;
}

// Emit deopt handler code.
int HandlerImpl::emit_deopt_handler(CodeBuffer& cbuf) {
  // Note that the code buffer's insts_mark is always relative to insts.
  // That's why we must use the macroassembler to generate a handler.
  MacroAssembler _masm(&cbuf);
  address base = __ start_a_stub(size_deopt_handler());
  if (base == NULL) {
    ciEnv::current()->record_failure("CodeCache is full");
    return 0;  // CodeBuffer::expand failed
  }

  int offset = __ offset();

  __ block_comment("; emit_deopt_handler");

  cbuf.set_insts_mark();
//  __ relocate(relocInfo::runtime_call_type);
  __ jump64_patchable(SharedRuntime::deopt_blob()->unpack(), relocInfo::runtime_call_type, true);
  __ align(16);
  assert(__ offset() - offset <= (int) size_deopt_handler(), "overflow");
  __ end_a_stub();
  return offset;
}


const bool Matcher::match_rule_supported(int opcode) {
  if (!has_match_rule(opcode))
    return false;

  switch (opcode) {
    //Op_CountLeadingZerosI Op_CountLeadingZerosL can be deleted, all MIPS CPUs support clz & dclz.
    case Op_CountLeadingZerosI:
    case Op_CountLeadingZerosL:
      break;
    case Op_CountTrailingZerosI:
    case Op_CountTrailingZerosL:
      break;
  }

  return true;  // Per default match rules are supported.
}

//FIXME
// emit call stub, compiled java to interpreter
void emit_java_to_interp(CodeBuffer &cbuf ) {
  // Stub is fixed up when the corresponding call is converted from calling
  // compiled code to calling interpreted code.
  // mov rbx,0
  // jmp -1
  Unimplemented();
  #if 0
  address mark = cbuf.insts_mark();  // get mark within main instrs section

  // Note that the code buffer's insts_mark is always relative to insts.
  // That's why we must use the macroassembler to generate a stub.
  MacroAssembler _masm(&cbuf);

  address base = __ start_a_stub(Compile::MAX_stubs_size);
  if (base == NULL) { // CodeBuffer::expand failed
    ciEnv::current()->record_failure("CodeCache is full");
  }

  // static stub relocation stores the instruction address of the call

  __ relocate(static_stub_Relocation::spec(mark), 0);

  // static stub relocation also tags the methodOop in the code-stream.
  __ patchable_set48(S3, (long)0);
  // This is recognized as unresolved by relocs/nativeInst/ic code

//  __ relocate(relocInfo::runtime_call_type);

  cbuf.set_insts_mark();
  address call_pc = (address)-1;
  __ jump64_patchable(call_pc, relocInfo::runtime_call_type, false);
  __ align(16);
  __ end_a_stub();
  // Update current stubs pointer and restore code_end.
  #endif
}

// size of call stub, compiled java to interpretor
uint size_java_to_interp() {
  int size = 4 * 4 + NativeCall::instruction_size; // sizeof(li48) + NativeCall::instruction_size
  return round_to(size, 16);
}

// relocation entries for call stub, compiled java to interpreter
uint reloc_java_to_interp() {
  return 16;  //  in emit_java_to_interp +  in Java_Static_Call
}

bool Matcher::is_short_branch_offset(int rule, int br_size, int offset) {
  int offs = offset - br_size + 4;
  // To be conservative on MIPS
  // branch node should be end with:
  //   branch inst
  //   delay slot
  const int safety_zone = 3 * BytesPerInstWord;
  return Assembler::is_simm16((offs<0 ? offs-safety_zone : offs+safety_zone) >> 2);
}


// No additional cost for CMOVL.
const int Matcher::long_cmove_cost() { return 0; }

// No CMOVF/CMOVD with SSE2
const int Matcher::float_cmove_cost() { return ConditionalMoveLimit; }

// Does the CPU require late expand (see block.cpp for description of late expand)?
const bool Matcher::require_postalloc_expand = false;

// Should the Matcher clone shifts on addressing modes, expecting them
// to be subsumed into complex addressing expressions or compute them
// into registers?  True for Intel but false for most RISCs
const bool Matcher::clone_shift_expressions = false;

// Do we need to mask the count passed to shift instructions or does
// the cpu only look at the lower 5/6 bits anyway?
const bool Matcher::need_masked_shift_count = false;

bool Matcher::narrow_oop_use_complex_address() {
  NOT_LP64(ShouldNotCallThis());
  assert(UseCompressedOops, "only for compressed oops code");
  return false;
}

bool Matcher::narrow_klass_use_complex_address() {
  NOT_LP64(ShouldNotCallThis());
  assert(UseCompressedClassPointers, "only for compressed klass code");
  return false;
}

// This is UltraSparc specific, true just means we have fast l2f conversion
const bool Matcher::convL2FSupported(void) {
  return true;
}

// Max vector size in bytes. 0 if not supported.
const int Matcher::vector_width_in_bytes(BasicType bt) {
  if (MaxVectorSize == 0)
    return 0;
  assert(MaxVectorSize == 8, "");
  return 8;
}

// Vector ideal reg
const uint Matcher::vector_ideal_reg(int size) {
  assert(MaxVectorSize == 8, "");
  switch(size) {
    case  8: return Op_VecD;
  }
  ShouldNotReachHere();
  return 0;
}

// Only lowest bits of xmm reg are used for vector shift count.
const uint Matcher::vector_shift_count_ideal_reg(int size) {
  fatal("vector shift is not supported");
  return Node::NotAMachineReg;
}

// Limits on vector size (number of elements) loaded into vector.
const int Matcher::max_vector_size(const BasicType bt) {
  assert(is_java_primitive(bt), "only primitive type vectors");
  return vector_width_in_bytes(bt)/type2aelembytes(bt);
}

const int Matcher::min_vector_size(const BasicType bt) {
  return max_vector_size(bt); // Same as max.
}

// MIPS supports misaligned vectors store/load? FIXME
const bool Matcher::misaligned_vectors_ok() {
  return false;
  //return !AlignVector; // can be changed by flag
}

// Register for DIVI projection of divmodI
RegMask Matcher::divI_proj_mask() {
  ShouldNotReachHere();
  return RegMask();
}

// Register for MODI projection of divmodI
RegMask Matcher::modI_proj_mask() {
  ShouldNotReachHere();
  return RegMask();
}

// Register for DIVL projection of divmodL
RegMask Matcher::divL_proj_mask() {
  ShouldNotReachHere();
  return RegMask();
}

int Matcher::regnum_to_fpu_offset(int regnum) {
  return regnum - 32; // The FP registers are in the second chunk
}


const bool Matcher::isSimpleConstant64(jlong value) {
  // Will one (StoreL ConL) be cheaper than two (StoreI ConI)?.
  return true;
}


// Return whether or not this register is ever used as an argument.  This
// function is used on startup to build the trampoline stubs in generateOptoStub.
// Registers not mentioned will be killed by the VM call in the trampoline, and
// arguments in those registers not be available to the callee.
bool Matcher::can_be_java_arg( int reg ) {
  /* Refer to: [sharedRuntime_mips_64.cpp] SharedRuntime::java_calling_convention() */
  if (    reg == X5_num || reg == X5_H_num
       || reg == X6_num || reg == X6_H_num
       || reg == X7_num || reg == X7_H_num
       || reg == X28_num || reg == X28_H_num
       || reg == X29_num || reg == X29_H_num
       || reg == X30_num || reg == X30_H_num
       || reg == X31_num || reg == X31_H_num
       || reg == X10_num || reg == X10_H_num
       || reg == X11_num || reg == X11_H_num
       || reg == X12_num || reg == X12_H_num
       || reg == X13_num || reg == X13_H_num
       || reg == X14_num || reg == X14_H_num
       || reg == X15_num || reg == X15_H_num
       || reg == X16_num || reg == X16_H_num
       || reg == X17_num || reg == X17_H_num )
    return true;

  if (    reg == F12_num || reg == F12_H_num
       || reg == F13_num || reg == F13_H_num
       || reg == F14_num || reg == F14_H_num
       || reg == F15_num || reg == F15_H_num
       || reg == F16_num || reg == F16_H_num
       || reg == F17_num || reg == F17_H_num
       || reg == F18_num || reg == F18_H_num
       || reg == F19_num || reg == F19_H_num )
    return true;

  return false;
}

bool Matcher::is_spillable_arg( int reg ) {
  return can_be_java_arg(reg);
}

bool Matcher::use_asm_for_ldiv_by_con( jlong divisor ) {
  return false;
}

// Register for MODL projection of divmodL
RegMask Matcher::modL_proj_mask() {
  ShouldNotReachHere();
  return RegMask();
}

const RegMask Matcher::method_handle_invoke_SP_save_mask() {
  return FP_REG_mask();
}

// MIPS doesn't support AES intrinsics
const bool Matcher::pass_original_key_for_aes() {
  return false;
}


// If CPU can load and store mis-aligned doubles directly then no fixup is
// needed.  Else we split the double into 2 integer pieces and move it
// piece-by-piece.  Only happens when passing doubles into C code as the
// Java calling convention forces doubles to be aligned.
const bool Matcher::misaligned_doubles_ok = false;
// Do floats take an entire double register or just half?
//const bool Matcher::float_in_double = true;
bool Matcher::float_in_double() { return false; }
// Threshold size for cleararray.
const int Matcher::init_array_short_size = 8 * BytesPerLong;
// Do ints take an entire long register or just half?
const bool Matcher::int_in_long = true;
// Is it better to copy float constants, or load them directly from memory?
// Intel can load a float constant from a direct address, requiring no
// extra registers.  Most RISCs will have to materialize an address into a
// register first, so they would do better to copy the constant from stack.
const bool Matcher::rematerialize_float_constants = false;
// Advertise here if the CPU requires explicit rounding operations
// to implement the UseStrictFP mode.
const bool Matcher::strict_fp_requires_explicit_rounding = false;
// The ecx parameter to rep stos for the ClearArray node is in dwords.
const bool Matcher::init_array_count_is_in_bytes = false;


// Indicate if the safepoint node needs the polling page as an input.
// Since MIPS doesn't have absolute addressing, it needs.
bool SafePointNode::needs_polling_address_input() {
  return false;
}

// !!!!! Special hack to get all type of calls to specify the byte offset
//       from the start of the call to the point where the return address
//       will point.
int MachCallStaticJavaNode::ret_addr_offset() {
  //lui
  //ori
  //nop
  //nop
  //jalr
  //nop
  return 24;
}

int MachCallDynamicJavaNode::ret_addr_offset() {
  //lui IC_Klass,
  //ori IC_Klass,
  //dsll IC_Klass
  //ori IC_Klass

  //lui T9
  //ori T9
  //nop
  //nop
  //jalr T9
  //nop
  return 4 * 4 + 4 * 6;
}

//=============================================================================

// Figure out which register class each belongs in: rc_int, rc_float, rc_stack
enum RC { rc_bad, rc_int, rc_float, rc_stack };
static enum RC rc_class( OptoReg::Name reg ) {
  if( !OptoReg::is_valid(reg)  ) return rc_bad;
  if (OptoReg::is_stack(reg)) return rc_stack;
  VMReg r = OptoReg::as_VMReg(reg);
  if (r->is_Register()) return rc_int;
  assert(r->is_FloatRegister(), "must be");
  return rc_float;
}

uint MachSpillCopyNode::implementation( CodeBuffer *cbuf, PhaseRegAlloc *ra_, bool do_size, outputStream* st ) const {
  // Get registers to move
  OptoReg::Name src_second = ra_->get_reg_second(in(1));
  OptoReg::Name src_first = ra_->get_reg_first(in(1));
  OptoReg::Name dst_second = ra_->get_reg_second(this );
  OptoReg::Name dst_first = ra_->get_reg_first(this );

  enum RC src_second_rc = rc_class(src_second);
  enum RC src_first_rc = rc_class(src_first);
  enum RC dst_second_rc = rc_class(dst_second);
  enum RC dst_first_rc = rc_class(dst_first);

  assert(OptoReg::is_valid(src_first) && OptoReg::is_valid(dst_first), "must move at least 1 register" );

  // Generate spill code!
  int size = 0;
  Unimplemented();
  #if 0
  if( src_first == dst_first && src_second == dst_second )
    return 0;            // Self copy, no move

  if (src_first_rc == rc_stack) {
    // mem ->
    if (dst_first_rc == rc_stack) {
      // mem -> mem
      assert(src_second != dst_first, "overlap");
      if ((src_first & 1) == 0 && src_first + 1 == src_second &&
          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {
        // 64-bit
        int src_offset = ra_->reg2offset(src_first);
        int dst_offset = ra_->reg2offset(dst_first);
        if (cbuf) {
          MacroAssembler _masm(cbuf);
          __ ld(AT, Address(SP, src_offset));
          __ sd(AT, Address(SP, dst_offset));
#ifndef PRODUCT
        } else {
          if(!do_size){
            if (size != 0) st->print("\n\t");
              st->print("ld    AT, [SP + #%d]\t# 64-bit mem-mem spill 1\n\t"
                        "sd    AT, [SP + #%d]",
                        src_offset, dst_offset);
          }
#endif
        }
        size += 8;
      } else {
        // 32-bit
        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), "no transform");
        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), "no transform");
        // No pushl/popl, so:
        int src_offset = ra_->reg2offset(src_first);
        int dst_offset = ra_->reg2offset(dst_first);
        if (cbuf) {
          MacroAssembler _masm(cbuf);
          __ lw(AT, Address(SP, src_offset));
          __ sw(AT, Address(SP, dst_offset));
#ifndef PRODUCT
        } else {
          if(!do_size){
            if (size != 0) st->print("\n\t");
              st->print("lw    AT, [SP + #%d] spill 2\n\t"
                        "sw    AT, [SP + #%d]\n\t",
                        src_offset, dst_offset);
          }
#endif
        }
        size += 8;
      }
      return size;
    } else if (dst_first_rc == rc_int) {
      // mem -> gpr
      if ((src_first & 1) == 0 && src_first + 1 == src_second &&
          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {
        // 64-bit
        int offset = ra_->reg2offset(src_first);
        if (cbuf) {
          MacroAssembler _masm(cbuf);
          __ ld(as_Register(Matcher::_regEncode[dst_first]), Address(SP, offset));
#ifndef PRODUCT
        } else {
          if(!do_size){
            if (size != 0) st->print("\n\t");
              st->print("ld    %s, [SP + #%d]\t# spill 3",
                        Matcher::regName[dst_first],
                        offset);
          }
#endif
        }
        size += 4;
      } else {
        // 32-bit
        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), "no transform");
        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), "no transform");
        int offset = ra_->reg2offset(src_first);
        if (cbuf) {
          MacroAssembler _masm(cbuf);
          if (this->ideal_reg() == Op_RegI)
            __ lw(as_Register(Matcher::_regEncode[dst_first]), Address(SP, offset));
          else
            __ lwu(as_Register(Matcher::_regEncode[dst_first]), Address(SP, offset));
#ifndef PRODUCT
          } else {
            if(!do_size){
              if (size != 0) st->print("\n\t");
              if (this->ideal_reg() == Op_RegI)
                st->print("lw    %s, [SP + #%d]\t# spill 4",
                          Matcher::regName[dst_first],
                          offset);
              else
                st->print("lwu    %s, [SP + #%d]\t# spill 5",
                          Matcher::regName[dst_first],
                          offset);
            }
#endif
          }
          size += 4;
      }
      return size;
    } else if (dst_first_rc == rc_float) {
      // mem-> xmm
      if ((src_first & 1) == 0 && src_first + 1 == src_second &&
          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {
        // 64-bit
        int offset = ra_->reg2offset(src_first);
        if (cbuf) {
          MacroAssembler _masm(cbuf);
          __ ldc1( as_FloatRegister(Matcher::_regEncode[dst_first]), Address(SP, offset));
#ifndef PRODUCT
        } else {
          if (!do_size) {
            if (size != 0) st->print("\n\t");
            st->print("ldc1  %s, [SP + #%d]\t# spill 6",
                      Matcher::regName[dst_first],
                      offset);
          }
#endif
        }
        size += 4;
      } else {
        // 32-bit
        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), "no transform");
        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), "no transform");
        int offset = ra_->reg2offset(src_first);
        if (cbuf) {
          MacroAssembler _masm(cbuf);
          __ lwc1( as_FloatRegister(Matcher::_regEncode[dst_first]), Address(SP, offset));
#ifndef PRODUCT
        } else {
          if(!do_size){
            if (size != 0) st->print("\n\t");
            st->print("lwc1   %s, [SP + #%d]\t# spill 7",
                      Matcher::regName[dst_first],
                      offset);
            }
#endif
        }
        size += 4;
      }
      return size;
    }
  } else if (src_first_rc == rc_int) {
    // gpr ->
    if (dst_first_rc == rc_stack) {
      // gpr -> mem
      if ((src_first & 1) == 0 && src_first + 1 == src_second &&
          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {
        // 64-bit
        int offset = ra_->reg2offset(dst_first);
        if (cbuf) {
          MacroAssembler _masm(cbuf);
          __ sd(as_Register(Matcher::_regEncode[src_first]), Address(SP, offset));
#ifndef PRODUCT
        } else {
          if(!do_size){
            if (size != 0) st->print("\n\t");
            st->print("sd    %s, [SP + #%d] # spill 8",
                      Matcher::regName[src_first],
                      offset);
          }
#endif
        }
        size += 4;
      } else {
        // 32-bit
        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), "no transform");
        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), "no transform");
        int offset = ra_->reg2offset(dst_first);
        if (cbuf) {
          MacroAssembler _masm(cbuf);
          __ sw(as_Register(Matcher::_regEncode[src_first]), Address(SP, offset));
#ifndef PRODUCT
        } else {
          if (!do_size) {
            if (size != 0) st->print("\n\t");
            st->print("sw    %s, [SP + #%d]\t# spill 9",
                      Matcher::regName[src_first], offset);
          }
#endif
        }
        size += 4;
      }
      return size;
    } else if (dst_first_rc == rc_int) {
      // gpr -> gpr
      if ((src_first & 1) == 0 && src_first + 1 == src_second &&
          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {
        // 64-bit
        if (cbuf) {
          MacroAssembler _masm(cbuf);
          __ move(as_Register(Matcher::_regEncode[dst_first]),
                  as_Register(Matcher::_regEncode[src_first]));
#ifndef PRODUCT
        } else {
          if(!do_size){
            if (size != 0) st->print("\n\t");
            st->print("move(64bit)    %s <-- %s\t# spill 10",
                      Matcher::regName[dst_first],
                      Matcher::regName[src_first]);
          }
#endif
        }
        size += 4;
        return size;
      } else {
        // 32-bit
        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), "no transform");
        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), "no transform");
        if (cbuf) {
          MacroAssembler _masm(cbuf);
          if (this->ideal_reg() == Op_RegI)
              __ move_u32(as_Register(Matcher::_regEncode[dst_first]), as_Register(Matcher::_regEncode[src_first]));
          else
              __ daddu(as_Register(Matcher::_regEncode[dst_first]), as_Register(Matcher::_regEncode[src_first]), R0);
#ifndef PRODUCT
        } else {
          if (!do_size) {
            if (size != 0) st->print("\n\t");
            st->print("move(32-bit)    %s <-- %s\t# spill 11",
                      Matcher::regName[dst_first],
                      Matcher::regName[src_first]);
          }
#endif
        }
        size += 4;
        return size;
      }
    } else if (dst_first_rc == rc_float) {
      // gpr -> xmm
      if ((src_first & 1) == 0 && src_first + 1 == src_second &&
          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {
        // 64-bit
        if (cbuf) {
          MacroAssembler _masm(cbuf);
          __ dmtc1(as_Register(Matcher::_regEncode[src_first]), as_FloatRegister(Matcher::_regEncode[dst_first]));
#ifndef PRODUCT
        } else {
          if(!do_size){
            if (size != 0) st->print("\n\t");
            st->print("dmtc1   %s, %s\t# spill 12",
                      Matcher::regName[dst_first],
                      Matcher::regName[src_first]);
          }
#endif
        }
        size += 4;
      } else {
        // 32-bit
        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), "no transform");
        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), "no transform");
        if (cbuf) {
          MacroAssembler _masm(cbuf);
          __ mtc1( as_Register(Matcher::_regEncode[src_first]), as_FloatRegister(Matcher::_regEncode[dst_first]) );
#ifndef PRODUCT
        } else {
          if(!do_size){
            if (size != 0) st->print("\n\t");
            st->print("mtc1   %s, %s\t# spill 13",
                      Matcher::regName[dst_first],
                      Matcher::regName[src_first]);
          }
#endif
        }
        size += 4;
      }
      return size;
    }
  } else if (src_first_rc == rc_float) {
    // xmm ->
    if (dst_first_rc == rc_stack) {
      // xmm -> mem
      if ((src_first & 1) == 0 && src_first + 1 == src_second &&
          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {
        // 64-bit
        int offset = ra_->reg2offset(dst_first);
        if (cbuf) {
          MacroAssembler _masm(cbuf);
          __ sdc1( as_FloatRegister(Matcher::_regEncode[src_first]), Address(SP, offset) );
#ifndef PRODUCT
        } else {
          if(!do_size){
            if (size != 0) st->print("\n\t");
            st->print("sdc1   %s, [SP + #%d]\t# spill 14",
                      Matcher::regName[src_first],
                      offset);
          }
#endif
        }
        size += 4;
      } else {
        // 32-bit
        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), "no transform");
        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), "no transform");
        int offset = ra_->reg2offset(dst_first);
        if (cbuf) {
          MacroAssembler _masm(cbuf);
          __ swc1(as_FloatRegister(Matcher::_regEncode[src_first]), Address(SP, offset));
#ifndef PRODUCT
        } else {
          if(!do_size){
            if (size != 0) st->print("\n\t");
            st->print("swc1   %s, [SP + #%d]\t# spill 15",
                      Matcher::regName[src_first],
                      offset);
          }
#endif
        }
        size += 4;
      }
      return size;
    } else if (dst_first_rc == rc_int) {
      // xmm -> gpr
      if ((src_first & 1) == 0 && src_first + 1 == src_second &&
          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {
        // 64-bit
        if (cbuf) {
          MacroAssembler _masm(cbuf);
          __ dmfc1( as_Register(Matcher::_regEncode[dst_first]), as_FloatRegister(Matcher::_regEncode[src_first]));
#ifndef PRODUCT
        } else {
          if(!do_size){
            if (size != 0) st->print("\n\t");
            st->print("dmfc1   %s, %s\t# spill 16",
                      Matcher::regName[dst_first],
                      Matcher::regName[src_first]);
          }
#endif
        }
        size += 4;
      } else {
        // 32-bit
        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), "no transform");
        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), "no transform");
        if (cbuf) {
          MacroAssembler _masm(cbuf);
          __ mfc1( as_Register(Matcher::_regEncode[dst_first]), as_FloatRegister(Matcher::_regEncode[src_first]));
#ifndef PRODUCT
        } else {
      if(!do_size){
            if (size != 0) st->print("\n\t");
            st->print("mfc1   %s, %s\t# spill 17",
                      Matcher::regName[dst_first],
                      Matcher::regName[src_first]);
          }
#endif
        }
        size += 4;
      }
      return size;
    } else if (dst_first_rc == rc_float) {
      // xmm -> xmm
      if ((src_first & 1) == 0 && src_first + 1 == src_second &&
          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {
        // 64-bit
        if (cbuf) {
          MacroAssembler _masm(cbuf);
          __ mov_d( as_FloatRegister(Matcher::_regEncode[dst_first]), as_FloatRegister(Matcher::_regEncode[src_first]));
#ifndef PRODUCT
        } else {
          if(!do_size){
            if (size != 0) st->print("\n\t");
            st->print("mov_d  %s <-- %s\t# spill 18",
                      Matcher::regName[dst_first],
                      Matcher::regName[src_first]);
          }
#endif
        }
        size += 4;
      } else {
        // 32-bit
        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), "no transform");
        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), "no transform");
        if (cbuf) {
          MacroAssembler _masm(cbuf);
          __ mov_s( as_FloatRegister(Matcher::_regEncode[dst_first]), as_FloatRegister(Matcher::_regEncode[src_first]));
#ifndef PRODUCT
        } else {
          if(!do_size){
            if (size != 0) st->print("\n\t");
            st->print("mov_s  %s <-- %s\t# spill 19",
                      Matcher::regName[dst_first],
                      Matcher::regName[src_first]);
          }
#endif
        }
        size += 4;
      }
      return size;
    }
  }
#endif

  assert(0," foo ");
  Unimplemented();
  return size;

}

#ifndef PRODUCT
void MachSpillCopyNode::format( PhaseRegAlloc *ra_, outputStream* st ) const {
  implementation( NULL, ra_, false, st );
}
#endif

void MachSpillCopyNode::emit(CodeBuffer &cbuf, PhaseRegAlloc *ra_) const {
  implementation( &cbuf, ra_, false, NULL );
}

uint MachSpillCopyNode::size(PhaseRegAlloc *ra_) const {
  return implementation( NULL, ra_, true, NULL );
}

//=============================================================================
#

#ifndef PRODUCT
void MachBreakpointNode::format( PhaseRegAlloc *, outputStream* st ) const {
  st->print("INT3");
}
#endif

void MachBreakpointNode::emit(CodeBuffer &cbuf, PhaseRegAlloc* ra_) const {
  MacroAssembler _masm(&cbuf);
  Unimplemented();
}

uint MachBreakpointNode::size(PhaseRegAlloc* ra_) const {
  return MachNode::size(ra_);
}


//=============================================================================
#ifndef PRODUCT
void MachEpilogNode::format( PhaseRegAlloc *ra_, outputStream* st ) const {
  Compile *C = ra_->C;
  int framesize = C->frame_size_in_bytes();

  assert((framesize & (StackAlignmentInBytes-1)) == 0, "frame size not aligned");

  st->print_cr("daddiu   SP, SP, %d # Rlease stack @ MachEpilogNode", framesize);
  st->print("\t");
  if (false /*UseLoongsonISA*/) {
    st->print_cr("gslq  RA, FP, SP, %d # Restore FP & RA @ MachEpilogNode", -wordSize*2);
  } else {
    st->print_cr("ld    RA, SP, %d # Restore RA @ MachEpilogNode", -wordSize);
    st->print("\t");
    st->print_cr("ld    FP, SP, %d # Restore FP @ MachEpilogNode", -wordSize*2);
  }

  if( do_polling() && C->is_method_compilation() ) {
    st->print("\t");
    st->print_cr("Poll Safepoint # MachEpilogNode");
  }
}
#endif

void MachEpilogNode::emit(CodeBuffer &cbuf, PhaseRegAlloc *ra_) const {
  Compile *C = ra_->C;
  MacroAssembler _masm(&cbuf);
  int framesize = C->frame_size_in_bytes();

  assert((framesize & (StackAlignmentInBytes-1)) == 0, "frame size not aligned");
  Unimplemented();
  #if 0
  __ daddiu(SP, SP, framesize);

  if (false /*UseLoongsonISA*/) {
    __ gslq(RA, FP, SP, -wordSize*2);
  } else {
    __ ld(RA, SP, -wordSize );
    __ ld(FP, SP, -wordSize*2 );
  }

  if( do_polling() && C->is_method_compilation() ) {
    __ set64(AT, (long)os::get_polling_page());
    __ relocate(relocInfo::poll_return_type);
    __ lw(AT, AT, 0);
  }
  #endif
}

uint MachEpilogNode::size(PhaseRegAlloc *ra_) const {
  return MachNode::size(ra_); // too many variables; just compute it the hard way  fujie debug
}

int MachEpilogNode::reloc() const {
  return 0; // a large enough number
}

const Pipeline * MachEpilogNode::pipeline() const {
  return MachNode::pipeline_class();
}

int MachEpilogNode::safepoint_offset() const { return 0; }

//=============================================================================

#ifndef PRODUCT
void BoxLockNode::format( PhaseRegAlloc *ra_, outputStream* st ) const {
  int offset = ra_->reg2offset(in_RegMask(0).find_first_elem());
  int reg = ra_->get_reg_first(this);
  st->print("ADDI %s, SP, %d   @BoxLockNode",Matcher::regName[reg],offset);
}
#endif


uint BoxLockNode::size(PhaseRegAlloc *ra_) const {
  return 4;
}

void BoxLockNode::emit(CodeBuffer &cbuf, PhaseRegAlloc *ra_) const {
  MacroAssembler _masm(&cbuf);
  int offset = ra_->reg2offset(in_RegMask(0).find_first_elem());
  int reg = ra_->get_encode(this);
  Unimplemented();
  //__ addi(as_Register(reg), SP, offset);
}


//static int sizeof_FFree_Float_Stack_All = -1;

int MachCallRuntimeNode::ret_addr_offset() {
  //lui
  //ori
  //dsll
  //ori
  //jalr
  //nop
  assert(NativeCall::instruction_size == 24, "in MachCallRuntimeNode::ret_addr_offset()");
  return NativeCall::instruction_size;
}


//=============================================================================
#ifndef PRODUCT
void MachNopNode::format( PhaseRegAlloc *, outputStream* st ) const {
  st->print("NOP \t# %d bytes pad for loops and calls", 4 * _count);
}
#endif

void MachNopNode::emit(CodeBuffer &cbuf, PhaseRegAlloc * ) const {
  MacroAssembler _masm(&cbuf);
  int i = 0;
  for(i = 0; i < _count; i++)
     __ nop();
}

uint MachNopNode::size(PhaseRegAlloc *) const {
  return 4 * _count;
}
const Pipeline* MachNopNode::pipeline() const {
  return MachNode::pipeline_class();
}

//=============================================================================

//=============================================================================
#ifndef PRODUCT
void MachUEPNode::format( PhaseRegAlloc *ra_, outputStream* st ) const {
  st->print_cr("load_klass(T9, T0)");
  st->print_cr("\tbeq(T9, iCache, L)");
  st->print_cr("\tnop");
  st->print_cr("\tjmp(SharedRuntime::get_ic_miss_stub(), relocInfo::runtime_call_type)");
  st->print_cr("\tnop");
  st->print_cr("\tnop");
  st->print_cr("    L:");
}
#endif


void MachUEPNode::emit(CodeBuffer &cbuf, PhaseRegAlloc *ra_) const {
  MacroAssembler _masm(&cbuf);
#ifdef ASSERT
  //uint code_size = cbuf.code_size();
#endif
  int  ic_reg = Matcher::inline_cache_reg_encode();
  Unimplemented();
  #if 0
  Label L;
  Register receiver = T0;
  Register   iCache = as_Register(ic_reg);
  __ load_klass(T9, receiver);
  __ beq(T9, iCache, L);
  __ delayed()->nop();

//  __ relocate(relocInfo::runtime_call_type);
  __ jump64_patchable((address)SharedRuntime::get_ic_miss_stub(), relocInfo::runtime_call_type, false);

  /* WARNING these NOPs are critical so that verified entry point is properly
   *      8 bytes aligned for patching by NativeJump::patch_verified_entry() */
  __ align(CodeEntryAlignment);
  __ bind(L);
  #endif
}

uint MachUEPNode::size(PhaseRegAlloc *ra_) const {
  return MachNode::size(ra_);
}



//=============================================================================

//const RegMask& MachConstantBaseNode::_out_RegMask = P_REG_mask();

int Compile::ConstantTable::calculate_table_base_offset() const {
  return 0;  // absolute addressing, no offset
}

bool MachConstantBaseNode::requires_postalloc_expand() const { return false; }
void MachConstantBaseNode::postalloc_expand(GrowableArray <Node *> *nodes, PhaseRegAlloc *ra_) {
  ShouldNotReachHere();
}

void MachConstantBaseNode::emit(CodeBuffer& cbuf, PhaseRegAlloc* ra_) const {
  Compile* C = ra_->C;
  Compile::ConstantTable& constant_table = C->constant_table();
  MacroAssembler _masm(&cbuf);

  Register Rtoc = as_Register(ra_->get_encode(this));
  CodeSection* consts_section = __ code()->consts();
  int consts_size = consts_section->align_at_start(consts_section->size());
  assert(constant_table.size() == consts_size, "must be equal");

  if (consts_section->size()) {
    // Materialize the constant table base.
    address baseaddr = consts_section->start() + -(constant_table.table_base_offset());
    // RelocationHolder rspec = internal_word_Relocation::spec(baseaddr);
    __ relocate(relocInfo::internal_word_type);
    //__ patchable_set48(Rtoc, (long)baseaddr);
  }
}

uint MachConstantBaseNode::size(PhaseRegAlloc* ra_) const {
  // patchable_set48 (4 insts)
  return 4 * 4;
}

#ifndef PRODUCT
void MachConstantBaseNode::format(PhaseRegAlloc* ra_, outputStream* st) const {
  Register r = as_Register(ra_->get_encode(this));
  st->print("patchable_set48    %s, &constanttable (constant table base) @ MachConstantBaseNode", r->name());
}
#endif


//=============================================================================
#ifndef PRODUCT
void MachPrologNode::format( PhaseRegAlloc *ra_, outputStream* st ) const {
  Compile* C = ra_->C;

  int framesize = C->frame_size_in_bytes();
  int bangsize = C->bang_size_in_bytes();
  assert((framesize & (StackAlignmentInBytes-1)) == 0, "frame size not aligned");

  // Calls to C2R adapters often do not accept exceptional returns.
  // We require that their callers must bang for them.  But be careful, because
  // some VM calls (such as call site linkage) can use several kilobytes of
  // stack.  But the stack safety zone should account for that.
  // See bugs 4446381, 4468289, 4497237.
  if (C->need_stack_bang(bangsize)) {
    st->print_cr("# stack bang"); st->print("\t");
  }
  if (false /*UseLoongsonISA*/) {
    st->print("gssq     RA, FP, %d(SP)  @ MachPrologNode\n\t", -wordSize*2);
  } else {
    st->print("sd       RA, %d(SP)  @ MachPrologNode\n\t", -wordSize);
    st->print("sd       FP, %d(SP)  @ MachPrologNode\n\t", -wordSize*2);
  }
  st->print("daddiu   FP, SP, -%d \n\t", wordSize*2);
  st->print("daddiu   SP, SP, -%d \t",framesize);
}
#endif


void MachPrologNode::emit(CodeBuffer &cbuf, PhaseRegAlloc *ra_) const {
  Unimplemented();
  #if 0
  Compile* C = ra_->C;
  MacroAssembler _masm(&cbuf);

  int framesize = C->frame_size_in_bytes();
  int bangsize = C->bang_size_in_bytes();

  assert((framesize & (StackAlignmentInBytes-1)) == 0, "frame size not aligned");

  if (C->need_stack_bang(bangsize)) {
    __ generate_stack_overflow_check(bangsize);
  }

  if (false /*UseLoongsonISA*/) {
    __ gssq(RA, FP, SP, -wordSize*2);
  } else {
    __ sd(RA, SP, -wordSize);
    __ sd(FP, SP, -wordSize*2);
  }
  __ daddiu(FP, SP, -wordSize*2);
  __ daddiu(SP, SP, -framesize);
  __ nop(); // Make enough room for patch_verified_entry()
  __ nop();

  C->set_frame_complete(cbuf.insts_size());
  if (C->has_mach_constant_base_node()) {
    // NOTE: We set the table base offset here because users might be
    // emitted before MachConstantBaseNode.
    Compile::ConstantTable& constant_table = C->constant_table();
    constant_table.set_table_base_offset(constant_table.calculate_table_base_offset());
  }
 #endif
}


uint MachPrologNode::size(PhaseRegAlloc *ra_) const {
  return MachNode::size(ra_); // too many variables; just compute it the hard way
}

int MachPrologNode::reloc() const {
  return 0; // a large enough number
}

%}

//----------ENCODING BLOCK-----------------------------------------------------
// This block specifies the encoding classes used by the compiler to output
// byte streams.  Encoding classes generate functions which are called by
// Machine Instruction Nodes in order to generate the bit encoding of the
// instruction.  Operands specify their base encoding interface with the
// interface keyword.  There are currently supported four interfaces,
// REG_INTER, CONST_INTER, MEMORY_INTER, & COND_INTER.  REG_INTER causes an
// operand to generate a function which returns its register number when
// queried.   CONST_INTER causes an operand to generate a function which
// returns the value of the constant when queried.  MEMORY_INTER causes an
// operand to generate four functions which return the Base Register, the
// Index Register, the Scale Value, and the Offset Value of the operand when
// queried.  COND_INTER causes an operand to generate six functions which
// return the encoding code (ie - encoding bits for the instruction)
// associated with each basic boolean condition for a conditional instruction.
// Instructions specify two basic values for encoding.  They use the
// ins_encode keyword to specify their encoding class (which must be one of
// the class names specified in the encoding block), and they use the
// opcode keyword to specify, in order, their primary, secondary, and
// tertiary opcode.  Only the opcode sections which a particular instruction
// needs for encoding need to be specified.
encode %{

  //Load byte signed

%}


//---------MIPS FRAME--------------------------------------------------------------
// Definition of frame structure and management information.
//
//  S T A C K   L A Y O U T    Allocators stack-slot number
//                             |   (to get allocators register number
//  G  Owned by    |        |  v    add SharedInfo::stack0)
//  r   CALLER     |        |
//  o     |        +--------+      pad to even-align allocators stack-slot
//  w     V        |  pad0  |        numbers; owned by CALLER
//  t   -----------+--------+----> Matcher::_in_arg_limit, unaligned
//  h     ^        |   in   |  5
//        |        |  args  |  4   Holes in incoming args owned by SELF
//  |     |    old |        |  3
//  |     |     SP-+--------+----> Matcher::_old_SP, even aligned
//  v     |        |  ret   |  3   return address
//     Owned by    +--------+
//      Self       |  pad2  |  2   pad to align old SP
//        |        +--------+  1
//        |        | locks  |  0
//        |        +--------+----> SharedInfo::stack0, even aligned
//        |        |  pad1  | 11   pad to align new SP
//        |        +--------+
//        |        |        | 10
//        |        | spills |  9   spills
//        V        |        |  8   (pad0 slot for callee)
//      -----------+--------+----> Matcher::_out_arg_limit, unaligned
//        ^        |  out   |  7
//        |        |  args  |  6   Holes in outgoing args owned by CALLEE
//   Owned by  new |        |
//    Callee    SP-+--------+----> Matcher::_new_SP, even aligned
//                  |        |
//
// Note 1: Only region 8-11 is determined by the allocator.  Region 0-5 is
//         known from SELF's arguments and the Java calling convention.
//         Region 6-7 is determined per call site.
// Note 2: If the calling convention leaves holes in the incoming argument
//         area, those holes are owned by SELF.  Holes in the outgoing area
//         are owned by the CALLEE.  Holes should not be nessecary in the
//         incoming area, as the Java calling convention is completely under
//         the control of the AD file.  Doubles can be sorted and packed to
//         avoid holes.  Holes in the outgoing arguments may be nessecary for
//         varargs C calling conventions.
// Note 3: Region 0-3 is even aligned, with pad2 as needed.  Region 3-5 is
//         even aligned with pad0 as needed.
//         Region 6 is even aligned.  Region 6-7 is NOT even aligned;
//         region 6-11 is even aligned; it may be padded out more so that
//         the region from SP to FP meets the minimum stack alignment.
// Note 4: For I2C adapters, the incoming FP may not meet the minimum stack
//         alignment.  Region 11, pad1, may be dynamically extended so that
//         SP meets the minimum alignment.


frame %{

  stack_direction(TOWARDS_LOW);

  // These two registers define part of the calling convention
  // between compiled code and the interpreter.
  // SEE StartI2CNode::calling_convention & StartC2INode::calling_convention & StartOSRNode::calling_convention
  // for more information.

  inline_cache_reg(X31);                // Inline Cache Register
  interpreter_method_oop_reg(X27);      // Method Oop Register when calling interpreter

  // Optional: name the operand used by cisc-spilling to access [stack_pointer + offset]
  cisc_spilling_operand_name(indOffset32);

  // Number of stack slots consumed by locking an object
  // generate Compile::sync_stack_slots
#ifdef _LP64
  sync_stack_slots(2);
#else
  sync_stack_slots(1);
#endif

  frame_pointer(X2);

  // Interpreter stores its frame pointer in a register which is
  // stored to the stack by I2CAdaptors.
  // I2CAdaptors convert from interpreted java to compiled java.

  interpreter_frame_pointer(X8);

  // generate Matcher::stack_alignment
  stack_alignment(StackAlignmentInBytes);  //wordSize = sizeof(char*);

  // Number of stack slots between incoming argument block and the start of
  // a new frame.  The PROLOG must add this many slots to the stack.  The
  // EPILOG must remove this many slots.  Intel needs one slot for
  // return address.
  // generate Matcher::in_preserve_stack_slots
  //in_preserve_stack_slots(VerifyStackAtCalls + 2);  //Now VerifyStackAtCalls is defined as false ! Leave one stack slot for ra and fp
  in_preserve_stack_slots(4);  //Now VerifyStackAtCalls is defined as false ! Leave two stack slots for ra and fp

  // Number of outgoing stack slots killed above the out_preserve_stack_slots
  // for calls to C.  Supports the var-args backing area for register parms.
  varargs_C_out_slots_killed(0);

  // The after-PROLOG location of the return address.  Location of
  // return address specifies a type (REG or STACK) and a number
  // representing the register number (i.e. - use a register name) or
  // stack slot.
  // Ret Addr is on stack in slot 0 if no locks or verification or alignment.
  // Otherwise, it is above the locks and verification slot and alignment word
  //return_addr(STACK -1+ round_to(1+VerifyStackAtCalls+Compile::current()->sync()*Compile::current()->sync_stack_slots(),WordsPerLong));
  return_addr(REG X1);

  // Body of function which returns an integer array locating
  // arguments either in registers or in stack slots.  Passed an array
  // of ideal registers called "sig" and a "length" count.  Stack-slot
  // offsets are based on outgoing arguments, i.e. a CALLER setting up
  // arguments for a CALLEE.  Incoming stack arguments are
  // automatically biased by the preserve_stack_slots field above.


  // will generated to Matcher::calling_convention(OptoRegPair *sig, uint length, bool is_outgoing)
  // StartNode::calling_convention call this.
  calling_convention %{
    SharedRuntime::java_calling_convention(sig_bt, regs, length, false);
  %}




  // Body of function which returns an integer array locating
  // arguments either in registers or in stack slots.  Passed an array
  // of ideal registers called "sig" and a "length" count.  Stack-slot
  // offsets are based on outgoing arguments, i.e. a CALLER setting up
  // arguments for a CALLEE.  Incoming stack arguments are
  // automatically biased by the preserve_stack_slots field above.


  // SEE CallRuntimeNode::calling_convention for more information.
  c_calling_convention %{
   (void) SharedRuntime::c_calling_convention(sig_bt, regs, /*regs2=*/NULL, length);
  %}


  // Location of C & interpreter return values
  // register(s) contain(s) return value for Op_StartI2C and Op_StartOSR.
  // SEE Matcher::match.
  c_return_value %{
    assert( ideal_reg >= Op_RegI && ideal_reg <= Op_RegL, "only return normal values" );
                               /* -- , -- , Op_RegN, Op_RegI, Op_RegP, Op_RegF, Op_RegD, Op_RegL */
    static int lo[Op_RegL+1] = { 0, 0, X10_num,       X10_num,       X10_num,       F0_num,       F0_num,    X10_num };
    static int hi[Op_RegL+1] = { 0, 0, OptoReg::Bad, OptoReg::Bad, X10_H_num,     OptoReg::Bad, F0_H_num,  X10_H_num };
    return OptoRegPair(hi[ideal_reg],lo[ideal_reg]);
  %}

  // Location of return values
  // register(s) contain(s) return value for Op_StartC2I and Op_Start.
  // SEE Matcher::match.

  return_value %{
    assert( ideal_reg >= Op_RegI && ideal_reg <= Op_RegL, "only return normal values" );
                               /* -- , -- , Op_RegN, Op_RegI, Op_RegP, Op_RegF, Op_RegD, Op_RegL */
    static int lo[Op_RegL+1] = { 0, 0, X10_num,       X10_num,       X10_num,       F0_num,       F0_num,     X10_num };
    static int hi[Op_RegL+1] = { 0, 0, OptoReg::Bad, OptoReg::Bad, X10_H_num,     OptoReg::Bad, F0_H_num,   X10_H_num};
    return OptoRegPair(hi[ideal_reg],lo[ideal_reg]);
  %}

%}

//----------ATTRIBUTES---------------------------------------------------------
//----------Operand Attributes-------------------------------------------------
op_attrib op_cost(0);        // Required cost attribute

//----------Instruction Attributes---------------------------------------------
ins_attrib ins_cost(100);       // Required cost attribute
ins_attrib ins_size(32);         // Required size attribute (in bits)
ins_attrib ins_pc_relative(0);  // Required PC Relative flag
ins_attrib ins_short_branch(0); // Required flag: is this instruction a
                                // non-matching short branch variant of some
                                                            // long branch?
ins_attrib ins_alignment(4);    // Required alignment attribute (must be a power of 2)
                                // specifies the alignment that some part of the instruction (not
                                // necessarily the start) requires.  If > 1, a compute_padding()
                                // function must be provided for the instruction

//----------OPERANDS-----------------------------------------------------------
// Operand definitions must precede instruction definitions for correct parsing
// in the ADLC because operands constitute user defined types which are used in
// instruction definitions.


//----------PIPELINE-----------------------------------------------------------
// Rules which define the behavior of the target architectures pipeline.

pipeline %{

  //----------ATTRIBUTES---------------------------------------------------------
  attributes %{
    fixed_size_instructions;          // Fixed size instructions
    branch_has_delay_slot;      // branch have delay slot in gs2
    max_instructions_per_bundle = 1;     // 1 instruction per bundle
    max_bundles_per_cycle = 4;         // Up to 4 bundles per cycle
         bundle_unit_size=4;
    instruction_unit_size = 4;           // An instruction is 4 bytes long
    instruction_fetch_unit_size = 16;    // The processor fetches one line
    instruction_fetch_units = 1;         // of 16 bytes

    // List of nop instructions
    nops( MachNop );
  %}

  //----------RESOURCES----------------------------------------------------------
  // Resources are the functional units available to the machine

  resources(D1, D2, D3, D4, DECODE = D1 | D2 | D3| D4,  ALU1, ALU2,  ALU = ALU1 | ALU2,  FPU1, FPU2, FPU = FPU1 | FPU2,  MEM,  BR);

  //----------PIPELINE DESCRIPTION-----------------------------------------------
  // Pipeline Description specifies the stages in the machine's pipeline

  // IF: fetch
  // ID: decode
  // RD: read
  // CA: caculate
  // WB: write back
  // CM: commit

  pipe_desc(IF, ID, RD, CA, WB, CM);

%}

